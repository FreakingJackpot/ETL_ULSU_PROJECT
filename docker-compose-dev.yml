version: '3.8'
x-web-config: &web-config
  DJANGO_SECRET_KEY: ${DJANGO_SECRET_KEY}
  DJANGO_DEBUG: ${DJANGO_DEBUG}
  DJANGO_ALLOWED_HOSTS: ${DJANGO_ALLOWED_HOSTS}
  DJANGO_CSRF_TRUSTED_ORIGINS: ${DJANGO_CSRF_TRUSTED_ORIGINS}

  DB_NAME: ${DB_NAME}
  DB_USER: ${DB_USER}
  DB_PASS: ${DB_PASS}
  DB_HOST: ${DB_HOST}
  DB_PORT: ${DB_PORT}

  EXTERNAL_DB_NAME: ${EXTERNAL_DB_NAME}
  EXTERNAL_DB_USER: ${EXTERNAL_DB_USER}
  EXTERNAL_DB_PASS: ${EXTERNAL_DB_PASS}
  EXTERNAL_DB_HOST: ${EXTERNAL_DB_HOST}
  EXTERNAL_DB_PORT: ${EXTERNAL_DB_PORT}

  LOKI_HOST: ${LOKI_HOST}


x-airflow-config: &airflow-config
  <<: *web-config
  AIRFLOW__CORE__DAGS_FOLDER: /dags
  AIRFLOW__CORE__PLUGINS_FOLDER: /plugins
  AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://airflow:${DB_AIRFLOW_PASS}@postgres:5432/airflow
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'

  AIRFLOW__CORE__PARALLELISM: ${AIRFLOW_PARALLELISM}
  AIRFLOW__CORE__DAG_CONCURRENCY: ${AIRFLOW_DAG_CONCURRENCY}
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: ${AIRFLOW_MAX_ACTIVE_RUNS_PER_DAG}
  AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_CORE_LOAD_EXAMPLES}
  AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW_CORE_LOAD_DEFAULT_CONNECTIONS}

  AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_RETRY: ${AIRFLOW_EMAIL_DEFAULT_EMAIL_ON_RETRY}
  AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_FAILURE: ${AIRFLOW_EMAIL_DEFAULT_EMAIL_ON_FAILURE}

  AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${DB_AIRFLOW_PASS}@postgres/airflow
  AIRFLOW_DJANGO_PATH_TO_SETTINGS_PY: /app/covid_dashboard

  AIRFLOW__SCHEDULER__STATSD_ON: "True"
  AIRFLOW__SCHEDULER__STATSD_HOST: statsd-exporter
  AIRFLOW__SCHEDULER__STATSD_PORT: 8125
  AIRFLOW__SCHEDULER__STATSD_PREFIX: airflow

  AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8081

  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/opt/airflow/logs"
  AIRFLOW__LOGGING__LOGGING_LEVEL: "INFO" # DEBUG, INFO, WARNING, ERROR or CRITICAL.
  AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: "WARNING"
  AIRFLOW__LOGGING__LOG_FORMAT: "%(message)s"

  TELEPUSH_TOKEN: ${TELEPUSH_TOKEN}


x-airflow-base: &airflow-base
  build:
    context: ./
    dockerfile: ./airflow/Dockerfile
  restart: always
  volumes:
    - ./airflow/dags:/dags
    - ./airflow/plugins:/plugins
    - ./app:/app
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres
    container_name: postgres
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASS}
      POSTGRES_DB: ${DB_NAME}
      PGUSER: ${DB_USER}
      POSTGRES_AIRFLOW_USER: airflow
      POSTGRES_AIRFLOW_PASSWORD: ${DB_AIRFLOW_PASS}
      POSTGRES_AIRFLOW_DB: airflow
    ports:
      - 5432:5432
    volumes:
      - postgres_data:/var/lib/postgresql/data/
      - ./postgres:/docker-entrypoint-initdb.d
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow", "&","pg_isready","-U", "$DB_USER", "--dbname", "DB_NAME" ]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  external_postgres:
    image: freakingjackpot/etl_project:external_database
    container_name: external_postgres
    restart: always
    environment:
      POSTGRES_USER: ${EXTERNAL_DB_USER}
      POSTGRES_PASSWORD: ${EXTERNAL_DB_PASS}
      POSTGRES_DB: ${EXTERNAL_DB_NAME}
    ports:
      - 5433:5433
    command: -p 5433


  web:
    build:
      context: ./
      dockerfile: ./app/Dockerfile
    container_name: web
    restart: always
    command: python manage.py runserver 0.0.0.0:8000
    ports:
      - 8000:8000
    volumes:
      - ./app:/home/app/web/
    environment:
      <<: *web-config

  redis:
    container_name: redis
    image: redis:7.2.3-alpine
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow:
    <<: *airflow-base
    container_name: airflow
    environment:
      <<: *airflow-config
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: ${AIRFLOW_SCHEDULER_DAG_DIR_LIST_INTERVAL}
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: ${AIRFLOW_SCHEDULER_CATCHUP_BY_DEFAULT}
      AIRFLOW__SCHEDULER__MAX_THREADS: ${AIRFLOW_SCHEDULER_MAX_THREADS}

      AIRFLOW__WEBSERVER__LOG_FETCH_TIMEOUT_SEC: ${AIRFLOW_WEBSERVER_LOG_FETCH_TIMEOUT_SEC}


    command: >
      -c "sleep 10 &&
           /entrypoint db init &&
          (/entrypoint webserver &) &&
          (/entrypoint celery flower &) &&
           /entrypoint scheduler"

    ports:
      # Celery Flower
      - 5555:5555
      # Airflow Webserver
      - 8081:8081

    depends_on:
      - postgres
      - redis

  worker:
    <<: *airflow-base
    container_name: airflow-worker
    environment:
      <<: *airflow-config

    command: -c " sleep 10 && /entrypoint celery worker"

    depends_on:
      - airflow
      - postgres
      - redis
    ports:
      - 8793:8793

  telepush:
    container_name: telepush
    image: ghcr.io/muety/telepush
    environment:
      APP_TOKEN: ${BOTFATHER_TOKEN}
      APP_MODE: poll
    command: -port 8083
    volumes:
      - telepush_data:/srv/data
    ports:
      - 8083:8083
volumes:
  postgres_data:
  telepush_data:
